# Week 4: Analytics Engineering - BigQuery, DBT
Setting up dbt cloud for a BigQuery data warehouse, creating a project, developing and deploying the models to production.  

## Setup
I am using the dbt cloud account (free for 1 developer) for BigQuery.
### Resources
* [creating a dbt account](https://www.getdbt.com/signup/)
* [BigQuery OAuth setup](https://docs.getdbt.com/docs/collaborate/manage-access/set-up-bigquery-oauth)
* [Youtube video on dbt and BigQuery setup](https://www.youtube.com/watch?v=6zDTbM6OUcs&ab_channel=JieJenn)

### Setup: BigQuery
* Ensure there is a service account with BigQuery Admin permissions  
* The JSON key generated by this service account will be needed for dbt setup  
* Ensure the [BigQuery API is enabled](https://console.cloud.google.com/apis/api/bigquery.googleapis.com)  
* Have all [trip data](https://github.com/DataTalksClub/nyc-tlc-data/) present in BigQuery including green taxi trips (2019, 2020), yellow taxi trips (2019, 2020), and fhv (2019)  
* Create a sandbox, staging, and production dataset schema in BigQuery for dbt  

### Setup: dbt
* Create dbt account  
* Create project and link JSON key generated previously  
* Link GitHub, create new repository if necessary  
* Initialize Project  
* Edit dbt_project.yaml name: 'taxi_rides_ny', update the name under models section to match, delete models/example/portion    
* Under models folder create models/staging and models/core  

## dbt Basics  
### Compiling
Note that dbt uses a python language Jinja which it compiles and runs in the data warehouse.  
Eample:
```sql
---dbt sql model
{{
  config(materialized='table')
}}

SELECT  *
FROM    staging-source_table
WHERE   record_State = 'ACTIVE'
```
```sql
---compiled code
CREATE TABLE my_schema.my_model AS
(
  SELECT  *
  FROM    staging-source_table
  WHERE   record_State = 'ACTIVE'
)
```
### FROM clause
Transformation are done in select statements.  
Select statements have FROM clause like below:  
```sql
FROM  {{ source('staging','yellow_tripdate_2021_01') }}
```
These sources are defined in a yaml file which the models call from:  
```yml
sources:
    - name: staging
      database: production
      schema: trips_data_all

      loaded_at_field: record_loaded_at
      tables:
        - name: green_tripdata
        - name: yellow_tripdata
          freshness:
            error_after: {count: 6, period: hour}
```
Inside the source there is a freshness clause.  
As above you could run a freshness check that would check if data is fresh within the last 6 hours.  
  
You can also upload CSV stored in your repository under the seed folder.  
Seeds can be ran as:  
```bash
dbt seed -s file_name
```

You can also use references to reference underlying tables and views to automatically build dependencies and resolve the schema.  
```sql
--dbt sql model
WITH  green_data AS (
      SELECT  *,
              'Green' AS service_type
      FROM    {{ ref('stg_green_tripdata') }}
),
```

```sql
--compiled code
WITH  green_data AS (
      SELECT  *,
              'Green' AS service_type
      FROM    "my_project"."dbt_dev"."stg_green_tripdata"
),
```
## dbt development
### Models
Open project in dbt.  
Create models/staging/stg_green_tripdata.sql  
Create schema.yml in models/staging to take advantage of references instead of using a basic FROM statement.  
```yml
# schema.yml
version: 2

sources:
    - name: staging
      database: dtc-de-0315
      schema: trips_data_all

      tables:
          - name: green_tripdata_partitoned
          - name: yellow_tripdata_partitoned
```
Create first model referencing this schema.yml.  
```sql
{{ config(materialized='view') }}

SELECT  *
FROM    {{ source('staging','green_tripdata_partitoned') }}
LIMIT   100
```
Run the model in the dbt terminal.  
```bash
dbt run
```
*Note my spelling error 'partitoned' as opposed to 'partitioned', I did not feel like updating all my tables names and rolled with it*  

### Macros
* Use control structures in SQL
* Use environment variables in your dbt project for production deployments
* Operates on the results of one query to generate another query
* Abstract snippets of SQL into reusable macros  

Create new file in macros:  
marcos/get_payment_type_description.sql  
Add defintion:  
```sql
{# This macro returns the description of the payment_type #}

{% macro get_payment_type_description(payment_type) %}

    CASE  {{ payment_type }}
          WHEN 1 THEN 'Credit card'
          WHEN 2 THEN 'Cash'
          WHEN 3 THEN 'No charge'
          WHEN 4 THEN 'Dispute'
          WHEN 5 THEN 'Unknown'
          WHEN 6 THEN 'Voided trip'
    END

{% endmacro %}
```
How to use the macro:  
```sql
SELECT  {{ get_payment_type_description('payment-type') }} AS payment_type_description
FROM    {{ source('staging','green_tripdata_partitoned') }}
WHERE   vendorid IS NOT NULL
```
How macro is compiled:  
```sql
SELECT  CASE  {{ payment_type }}
              WHEN 1 THEN 'Credit card'
              WHEN 2 THEN 'Cash'
              WHEN 3 THEN 'No charge'
              WHEN 4 THEN 'Dispute'
              WHEN 5 THEN 'Unknown'
              WHEN 6 THEN 'Voided trip'
        END AS payment_type_description,
        congestion_surcharge::double precision
FROM    {{ source('staging','green_tripdata') }}
WHERE    vendorid IS NOT NULL
```

### Packages 
* Similar to libraries in python
* Standalone dbt projects, with their own models and macros
* If you add a package to your project the package's models and macros will become a part of your project
* Imported in packages.yml file and by running 'dbt deps'
* A list of useful packages can be found at [dbt package hub](https://hub.getdbt.com/)

Create new file in root under project folder packages.yml  
  
packages.yml contents:
```sql
packages:
  - package: dbt-labs/dbt_utils
    version: 0.8.0
```

Run command in dbt cloud to install:  
```bash
dbt deps
```

You can check what was installed under dbt_packages/dbt_utils  

Call it as a jinja function:  
```sql
{{ config(materialized='view') }}

SELECT  {{ dbt_utils.surrogate_key(['vendorid', 'lpep_pickup_datetime']) }} AS tripid
FROM    {{ source('staging','green_tripdata_partitoned') }}
WHERE   vendorid IS NOT NULL
```

### Variables
* Useful for defining valuables that should be used across the project
* allows us to provide data to models for compilation
* can be defined in 2 ways
    * in the dbt_project.yml file
    * on the command line  

Adding a variable for LIMIT 100 in our stg_green_tripdata.sql:  
```sql
{{ config(materialized='view') }}

SELECT  *
FROM    {{ source('staging','green_tripdata_partitoned') }}
WHERE   vendorid IS NOT NULL
{% if var('is_test_run', default=true) %}

    LIMIT 100

{% endif %}
```
```bash
dbt run
```
Note this defaults to TRUE and uses LIMIT 100  
We can set the var to false:  
```bash
dbt run --select stg_green_tripdata --var 'is_test_run:false'
```

## Putting it all together

### yellow and green tripdata

models/staging/[schema.yml](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/models/staging/staging.yml)  
models/staging/[stg_green_tripdata.sql](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/models/staging/stg_green_tripdata.sql)  
models/staging/[stg_yellow_tripdata.sql](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/models/staging/stg_yellow_tripdata.sql)  
