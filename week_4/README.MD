# Week 4: Analytics Engineering - BigQuery, DBT, Looker
Setting up dbt cloud for a BigQuery data warehouse, creating a project, developing and deploying the models to production. Visualizing the data using Looker.    

## Setup
I am using the dbt cloud account (free for 1 developer) for BigQuery.
### Resources
* [creating a dbt account](https://www.getdbt.com/signup/)
* [BigQuery OAuth setup](https://docs.getdbt.com/docs/collaborate/manage-access/set-up-bigquery-oauth)
* [Youtube video on dbt and BigQuery setup](https://www.youtube.com/watch?v=6zDTbM6OUcs&ab_channel=JieJenn)

### Setup: BigQuery
* Ensure there is a service account with BigQuery Admin permissions  
* The JSON key generated by this service account will be needed for dbt setup  
* Ensure the [BigQuery API is enabled](https://console.cloud.google.com/apis/api/bigquery.googleapis.com)  
* Have all [trip data](https://github.com/DataTalksClub/nyc-tlc-data/) present in BigQuery including green taxi trips (2019, 2020), yellow taxi trips (2019, 2020), and fhv (2019)  
* Create a sandbox, staging, and production dataset schema in BigQuery for dbt  

### Setup: dbt
* Create dbt account  
* Create project and link JSON key generated previously  
* Link GitHub, create new repository if necessary  
* Initialize Project  
* Edit dbt_project.yaml name: 'taxi_rides_ny', update the name under models section to match, delete models/example/portion    
* Under models folder create models/staging and models/core  

## dbt Basics  
### Compiling
Note that dbt uses a python language Jinja which it compiles and runs in the data warehouse.  
Eample:
```sql
---dbt sql model
{{
  config(materialized='table')
}}

SELECT  *
FROM    staging-source_table
WHERE   record_State = 'ACTIVE'
```
```sql
---compiled code
CREATE TABLE my_schema.my_model AS
(
  SELECT  *
  FROM    staging-source_table
  WHERE   record_State = 'ACTIVE'
)
```
### FROM clause
Transformation are done in select statements.  
Select statements have FROM clause like below:  
```sql
FROM  {{ source('staging','yellow_tripdate_2021_01') }}
```
These sources are defined in a yaml file which the models call from:  
```yml
sources:
    - name: staging
      database: production
      schema: trips_data_all

      loaded_at_field: record_loaded_at
      tables:
        - name: green_tripdata
        - name: yellow_tripdata
          freshness:
            error_after: {count: 6, period: hour}
```
Inside the source there is a freshness clause.  
As above you could run a freshness check that would check if data is fresh within the last 6 hours.  
  
You can also upload CSV stored in your repository under the seed folder.  
Seeds can be ran as:  
```bash
dbt seed -s file_name
```

You can also use references to reference underlying tables and views to automatically build dependencies and resolve the schema.  
```sql
--dbt sql model
WITH  green_data AS (
      SELECT  *,
              'Green' AS service_type
      FROM    {{ ref('stg_green_tripdata') }}
),
```

```sql
--compiled code
WITH  green_data AS (
      SELECT  *,
              'Green' AS service_type
      FROM    "my_project"."dbt_dev"."stg_green_tripdata"
),
```
## dbt development
### Models
Open project in dbt.  
Create models/staging/stg_green_tripdata.sql  
Create schema.yml in models/staging to take advantage of references instead of using a basic FROM statement.  
```yml
# schema.yml
version: 2

sources:
    - name: staging
      database: dtc-de-0315
      schema: trips_data_all

      tables:
          - name: green_tripdata_partitoned
          - name: yellow_tripdata_partitoned
```
Create first model referencing this schema.yml.  
```sql
{{ config(materialized='view') }}

SELECT  *
FROM    {{ source('staging','green_tripdata_partitoned') }}
LIMIT   100
```
Run the model in the dbt terminal.  
```bash
dbt run
```
*Note my spelling error 'partitoned' as opposed to 'partitioned', I did not feel like updating all my tables names and rolled with it*  

### Macros
* Use control structures in SQL
* Use environment variables in your dbt project for production deployments
* Operates on the results of one query to generate another query
* Abstract snippets of SQL into reusable macros  

Create new file in macros:  
marcos/get_payment_type_description.sql  
Add defintion:  
```sql
{# This macro returns the description of the payment_type #}

{% macro get_payment_type_description(payment_type) %}

    CASE  {{ payment_type }}
          WHEN 1 THEN 'Credit card'
          WHEN 2 THEN 'Cash'
          WHEN 3 THEN 'No charge'
          WHEN 4 THEN 'Dispute'
          WHEN 5 THEN 'Unknown'
          WHEN 6 THEN 'Voided trip'
    END

{% endmacro %}
```
How to use the macro:  
```sql
SELECT  {{ get_payment_type_description('payment-type') }} AS payment_type_description
FROM    {{ source('staging','green_tripdata_partitoned') }}
WHERE   vendorid IS NOT NULL
```
How macro is compiled:  
```sql
SELECT  CASE  {{ payment_type }}
              WHEN 1 THEN 'Credit card'
              WHEN 2 THEN 'Cash'
              WHEN 3 THEN 'No charge'
              WHEN 4 THEN 'Dispute'
              WHEN 5 THEN 'Unknown'
              WHEN 6 THEN 'Voided trip'
        END AS payment_type_description,
        congestion_surcharge::double precision
FROM    {{ source('staging','green_tripdata') }}
WHERE    vendorid IS NOT NULL
```

### Packages 
* Similar to libraries in python
* Standalone dbt projects, with their own models and macros
* If you add a package to your project the package's models and macros will become a part of your project
* Imported in packages.yml file and by running 'dbt deps'
* A list of useful packages can be found at [dbt package hub](https://hub.getdbt.com/)

Create new file in root under project folder packages.yml  
  
packages.yml contents:
```sql
packages:
  - package: dbt-labs/dbt_utils
    version: 0.8.0
```

Run command in dbt cloud to install:  
```bash
dbt deps
```

You can check what was installed under dbt_packages/dbt_utils  

Call it as a jinja function:  
```sql
{{ config(materialized='view') }}

SELECT  {{ dbt_utils.surrogate_key(['vendorid', 'lpep_pickup_datetime']) }} AS tripid
FROM    {{ source('staging','green_tripdata_partitoned') }}
WHERE   vendorid IS NOT NULL
```

### Variables
* Useful for defining valuables that should be used across the project
* allows us to provide data to models for compilation
* can be defined in 2 ways
    * in the dbt_project.yml file
    * on the command line  

Adding a variable for LIMIT 100 in our stg_green_tripdata.sql:  
```sql
{{ config(materialized='view') }}

SELECT  *
FROM    {{ source('staging','green_tripdata_partitoned') }}
WHERE   vendorid IS NOT NULL
{% if var('is_test_run', default=true) %}

    LIMIT 100

{% endif %}
```
```bash
dbt run
```
Note this defaults to TRUE and uses LIMIT 100  
We can set the var to false:  
```bash
dbt run --select stg_green_tripdata --var 'is_test_run:false'
```

### Seeds
You can use CSV files in your respositories as tables  
Create [seeds/taxi_zone_lookup.csv]()  
Copy from [source](https://github.com/DataTalksClub/data-engineering-zoomcamp/edit/main/week_4_analytics_engineering/taxi_rides_ny/data/taxi_zone_lookup.csv).  
Run dbt seed to create table:  
```bash
dbt seed
```
You can overwrite the dbt decided column types by adding them into the dbt_project.yml as below:  
```yml
seeds:
  taxi_rides_ny:
    taxi_zone_lookup:
      -column types:
        locationid: numeric
```
If values have been added to the CSV a full refresh is needed:  
```bash
dbt seed --full-refresh
```

Lets create a model from this CSV in core:  
models/core/dim_zones.sql  

```sql
{{ config(materialized='table') }}


SELECT  locationid, 
        borough, 
        zone, 
        REPLACE(service_zone,'Boro','Green') AS service_zone
FROM    {{ ref('taxi_zone_lookup') }}
```

Note 'dbt run' will not run seeds if you want to run everything including seeds use 'dbt build'  

### Tests and Documentation
#### Tests
* Assumptions that we make about our data
* Tests in dbt are essentially a select sql query
* Assumptions get compiled to sql that returns the amount of failing records vs the assumption
* Tests are defined in a column in the .yml file
* dbt provides basic tests:
  * Unique
  * Not null
  * Accepted values
  * A foreign key to another table
* You can create custom tests
#### Documentation
* dbt provides aw ay to generate documentation for your dbt project and render it as a website
* Documentation includes:
  * model code
  * model dependencies
  * sources
  * auto generated DAG from the ref and source macros
  * descriptions and tests
  * column names and data types
  * table stats like size and rows

For examples see the documentation and testing created in the schema.yml files below:  
models/staging/[schema.yml](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/models/staging/staging.yml)  
models/core/[schema.yml](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/models/core/schema.yml)

### Deployment
* Version control and CI/CD
* Running models created in the development environment in a production environment
* A deployment environment will normally have a different schema and ideally a different user
* Development-Deployment workflow
  * Develop in a user branch
  * Open a PR to merge into the main branch
  * Merge the branch to the main branch
  * Run the new models in the production environment using the main branch
  * Schedule the models
* dbt cloud includes a scheduler to create jobs
  * A single job can run multiple commands
  * Jobs can be triggered manually or on a schedule
  * Each job will keep a log of the runs over time
  * Each run will have the logs of each command
  * Jobs can also generate documentation that can be viewed under the run information
  * If dbt source freshness was run, the results can also be viewed at the end of a job
* dbt allows us to enable CI on pull requests
  * enabled via webhooks from GitHub or GitLab
  * When a PR is ready to be merged, a webhook is received in dbt cloud that will queue a new run of the specified job
  * The run of the CI job will be against a temporary schema
  * No PR will be able to be merged unless the run has been completed successfully  

In dbt cloud under project environments create a production environment.  
Previously everything was done in the development environment.  
Create the first job.  
Note the scheduling, CI/CD, and other features mentioned above.  
You can schedule multiple tasks ie:  
```bash
dbt seed
dbt run
dbt test
```
Once complete, save and run the job.  

### Putting it all together

models/staging/[schema.yml](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/models/staging/staging.yml)  
models/staging/[stg_green_tripdata.sql](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/models/staging/stg_green_tripdata.sql)  
models/staging/[stg_yellow_tripdata.sql](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/models/staging/stg_yellow_tripdata.sql)  
models/core/[schema.yml](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/models/core/schema.yml)
models/core/[dim_zones.sql](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/models/core/dim_zones.sql)  
models/core/[fact_trips.sql](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/models/core/fact_trips.sql)  
models/core/[dim_monthly_zone_revenue.sql](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/models/core/dim_monthly_zone_revenue.sql)  
[dbt_project.yml](https://github.com/TylerJSimpson/data_engineering_zoomcamp/blob/main/week_4/dbt_project.yml)  

## Visualization: Looker
Going to visualize the main fact table.  
In data sources choose BigQuery.  
DTC-DE/production/fact_trips  
Change default aggregations as necessary.  
Changing these to NULL:  
dropoff_locationid, pickup_locationid, ratecodeid, payment type, trip type, vendorid  
Create report.  
Please see an [example report](https://lookerstudio.google.com/s/kfnV1LcxmcI) created with the data.  
